apiVersion: batch/v1
kind: Job
metadata:
  name: ${TEST_NAME}
  namespace: default
  labels:
    test-batch: "${BATCH_SIZE}"
    test-steps: "${INFERENCE_STEPS}"
    test-instance: "${INSTANCE_TYPE}"
    test-precision: "${PRECISION}"
spec:
  template:
    spec:
      restartPolicy: Never
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      nodeSelector:
        accelerator: nvidia
        kubernetes.io/arch: amd64
        node.kubernetes.io/instance-type: ${INSTANCE_TYPE}
      containers:
      - name: stable-diffusion
        image: pytorch/pytorch:2.7.1-cuda12.8-cudnn9-runtime
        command:
          - /bin/bash
          - -c
          - |
            set -e
            echo "=== é€šç”¨Stable Diffusionæµ‹è¯•é…ç½® ==="
            echo "Test Name: ${TEST_NAME}"
            echo "Instance Type: ${INSTANCE_TYPE}"
            echo "Model: ${MODEL_ID}"
            echo "Batch Size: ${BATCH_SIZE}"
            echo "Inference Steps: ${INFERENCE_STEPS}"
            echo "Resolution: ${IMAGE_WIDTH}x${IMAGE_HEIGHT}"
            echo "Precision: ${PRECISION}"
            echo "Prompt: ${PROMPT}"
            
            echo "=== ç¯å¢ƒä¿¡æ¯ ==="
            echo "Hostname: $(hostname)"
            echo "PyTorch version: $(python -c 'import torch; print(torch.__version__)')"
            echo "CUDA available: $(python -c 'import torch; print(torch.cuda.is_available())')"
            if python -c 'import torch; exit(0 if torch.cuda.is_available() else 1)'; then
                echo "GPU count: $(python -c 'import torch; print(torch.cuda.device_count())')"
                echo "GPU name: $(python -c 'import torch; print(torch.cuda.get_device_name(0))')"
                echo "GPU memory: $(python -c 'import torch; print(str(round(torch.cuda.get_device_properties(0).total_memory/1024**3, 1)) + "GB")')"
                echo "CUDA version: $(python -c 'import torch; print(torch.version.cuda)')"
            else
                echo "CUDA not available!"
                exit 1
            fi
            
            echo "=== æ£€æŸ¥å­˜å‚¨æŒ‚è½½ ==="
            echo "EFS mount point: /shared"
            ls -la /shared/
            mkdir -p /shared/stable-diffusion-outputs
            echo "Created output directory: /shared/stable-diffusion-outputs"
            
            echo "=== å®‰è£…AIåº“ ==="
            pip install --no-cache-dir \
                diffusers \
                transformers \
                accelerate \
                safetensors
            
            echo "=== å¼€å§‹é€šç”¨Stable Diffusionæ¨ç†æµ‹è¯• ==="
            python -c "
            import torch
            from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline, DPMSolverMultistepScheduler
            import time
            import os
            import platform
            import signal
            import sys
            from datetime import datetime
            import json
            
            # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œæ•è·ç³»ç»Ÿçº§ç»ˆæ­¢ä¿¡å·
            def signal_handler(signum, frame):
                if signum == 15:  # SIGTERM - æ­£å¸¸ç»ˆæ­¢ä¿¡å·
                    print(f'âœ… æ”¶åˆ°ç»ˆæ­¢ä¿¡å· {signum} (SIGTERM)ï¼Œè„šæœ¬ä¸»åŠ¨å–æ¶ˆä»»åŠ¡')
                    print(f'ğŸ”„ å®¹å™¨æ­£å¸¸ç»ˆæ­¢ï¼Œæµ‹è¯•å·²å®Œæˆ')
                    # ä¸è¾“å‡ºOOMçš„PERFORMANCE_SUMMARYï¼Œè®©ä¹‹å‰çš„æ­£å¸¸ç»“æœä¿æŒæœ‰æ•ˆ
                elif signum == 9:  # SIGKILL - å¼ºåˆ¶ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯OOM
                    print(f'âŒ æ”¶åˆ°å¼ºåˆ¶ç»ˆæ­¢ä¿¡å· {signum} (SIGKILL)ï¼Œå¯èƒ½æ˜¯OOMå¯¼è‡´')
                    print(f'ğŸ”„ å®¹å™¨å› OOMè¢«å¼ºåˆ¶ç»ˆæ­¢')
                    print(f'PERFORMANCE_SUMMARY: {os.environ.get(\"TEST_NAME\", \"unknown\")},{os.environ.get(\"INSTANCE_TYPE\", \"unknown\")},{os.environ.get(\"BATCH_SIZE\", \"unknown\")},{os.environ.get(\"INFERENCE_STEPS\", \"unknown\")},\"N/A\",\"N/A\",OOM_SIGNAL')
                elif signum == 11:  # SIGSEGV - æ®µé”™è¯¯ï¼Œé€šå¸¸æ˜¯GPU OOMå¯¼è‡´
                    print(f'âŒ æ”¶åˆ°æ®µé”™è¯¯ä¿¡å· {signum} (SIGSEGV)ï¼Œå¯èƒ½æ˜¯GPUå†…å­˜ä¸è¶³å¯¼è‡´')
                    print(f'ğŸ”„ å®¹å™¨å› æ®µé”™è¯¯ç»ˆæ­¢ï¼Œé€šå¸¸æ˜¯GPU OOMæˆ–å†…å­˜è®¿é—®è¿è§„')
                    print(f'ğŸ’¡ å»ºè®®: ä½¿ç”¨float16ç²¾åº¦ã€å‡å°‘batch_sizeæˆ–é€‰æ‹©æ›´å¤§çš„GPUå®ä¾‹')
                    print(f'PERFORMANCE_SUMMARY: {os.environ.get(\"TEST_NAME\", \"unknown\")},{os.environ.get(\"INSTANCE_TYPE\", \"unknown\")},{os.environ.get(\"BATCH_SIZE\", \"unknown\")},{os.environ.get(\"INFERENCE_STEPS\", \"unknown\")},\"N/A\",\"N/A\",OOM_SIGSEGV')
                else:
                    print(f'âŒ æ”¶åˆ°ç³»ç»Ÿä¿¡å· {signum}ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸ç»ˆæ­¢')
                    print(f'ğŸ”„ å®¹å™¨å› ç³»ç»Ÿä¿¡å·ç»ˆæ­¢ï¼Œæ ‡è®°ä¸ºå¼‚å¸¸æƒ…å†µ')
                    print(f'PERFORMANCE_SUMMARY: {os.environ.get(\"TEST_NAME\", \"unknown\")},{os.environ.get(\"INSTANCE_TYPE\", \"unknown\")},{os.environ.get(\"BATCH_SIZE\", \"unknown\")},{os.environ.get(\"INFERENCE_STEPS\", \"unknown\")},\"N/A\",\"N/A\",SIGNAL_{signum}')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
                sys.exit(139)  # ä½¿ç”¨139é€€å‡ºç è¡¨ç¤ºæ®µé”™è¯¯/OOM
            
            # æ³¨å†Œä¿¡å·å¤„ç†å™¨
            signal.signal(signal.SIGTERM, signal_handler)  # Kubernetesç»ˆæ­¢ä¿¡å·
            signal.signal(signal.SIGINT, signal_handler)   # ä¸­æ–­ä¿¡å·
            signal.signal(signal.SIGSEGV, signal_handler)  # æ®µé”™è¯¯ä¿¡å· (GPU OOMå¸¸è§)
            try:
                signal.signal(signal.SIGUSR1, signal_handler)  # ç”¨æˆ·å®šä¹‰ä¿¡å·1
                signal.signal(signal.SIGUSR2, signal_handler)  # ç”¨æˆ·å®šä¹‰ä¿¡å·2
            except:
                pass  # æŸäº›ç³»ç»Ÿå¯èƒ½ä¸æ”¯æŒè¿™äº›ä¿¡å·
            
            # ä»ç¯å¢ƒå˜é‡è¯»å–æµ‹è¯•å‚æ•°
            test_name = os.environ.get('TEST_NAME', 'default')
            model_id = os.environ.get('MODEL_ID', 'stabilityai/stable-diffusion-2-1')
            instance_type = os.environ.get('INSTANCE_TYPE', 'unknown')
            batch_size = int(os.environ.get('BATCH_SIZE', '1'))
            inference_steps = int(os.environ.get('INFERENCE_STEPS', '20'))
            prompt = os.environ.get('PROMPT', 'a photo of an astronaut riding a horse on mars')
            width = int(os.environ.get('IMAGE_WIDTH', '1024'))
            height = int(os.environ.get('IMAGE_HEIGHT', '1024'))
            precision = os.environ.get('PRECISION', 'float32')
            
            print(f'Loading Stable Diffusion model: {model_id}')
            print(f'Test configuration: {test_name}')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºSDXLæ¨¡å‹
            is_sdxl = 'stable-diffusion-xl' in model_id.lower() or 'sdxl' in model_id.lower()
            print(f'Is SDXL model: {is_sdxl}')
            
            # è®°å½•æ¨¡å‹åŠ è½½æ—¶é—´
            model_load_start = time.time()
            
            # æ ¹æ®ç²¾åº¦è®¾ç½®torch_dtype
            if precision.lower() == 'float16' or precision.lower() == 'fp16':
                torch_dtype = torch.float16
                print('Using Float16 precision')
            elif precision.lower() == 'bfloat16' or precision.lower() == 'bf16':
                torch_dtype = torch.bfloat16
                print('Using BFloat16 precision')
            else:
                torch_dtype = torch.float32
                print('Using Float32 precision')
            
            # åŠ è½½æ¨¡å‹ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„Pipeline
            if is_sdxl:
                print('Loading SDXL model with StableDiffusionXLPipeline')
                pipe = StableDiffusionXLPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype,
                    use_safetensors=True,
                    variant='fp16' if torch_dtype == torch.float16 else None
                )
                # SDXL ä½¿ç”¨é»˜è®¤çš„è°ƒåº¦å™¨é€šå¸¸æ•ˆæœæ›´å¥½
                print(f'Using SDXL default scheduler: {pipe.scheduler.__class__.__name__}')
            else:
                print('Loading standard SD model with StableDiffusionPipeline')
                pipe = StableDiffusionPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype,
                    safety_checker=None,
                    requires_safety_checker=False
                )
                
                # ä½¿ç”¨DPM-Solver++è°ƒåº¦å™¨ï¼ˆé€‚ç”¨äºSD 2.xï¼‰
                if 'stable-diffusion-2' in model_id or 'sd-2' in model_id:
                    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
                    print('Using DPM-Solver++ scheduler for SD 2.x')
                else:
                    print(f'Using default scheduler: {pipe.scheduler.__class__.__name__}')
            
            # ç§»åŠ¨åˆ°GPU
            pipe = pipe.to('cuda')
            
            model_load_time = time.time() - model_load_start
            print(f'Model loaded in {model_load_time:.2f} seconds')
            print(f'MODEL_LOAD_TIME: {model_load_time:.2f}')  # ä¾¿äºè„šæœ¬è§£æ
            print(f'Model device: {pipe.device}')
            print(f'UNet device: {pipe.unet.device}')
            print(f'Scheduler: {pipe.scheduler.__class__.__name__}')
            
            print(f'Starting inference test:')
            print(f'  Test Name: {test_name}')
            print(f'  Model: {model_id}')
            print(f'  Instance Type: {instance_type}')
            print(f'  Batch Size: {batch_size}')
            print(f'  Inference Steps: {inference_steps}')
            print(f'  Resolution: {width}x{height}')
            print(f'  Precision: {precision} ({torch_dtype})')
            print(f'  Prompt: {prompt}')
            
            # å¼€å§‹æ¨ç†
            print('Starting inference...')
            sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
            inference_start = time.time()
            
            try:
                with torch.no_grad():
                    print(f'ğŸš€ å¼€å§‹ç”Ÿæˆ {batch_size} å¼ å›¾ç‰‡ï¼Œåˆ†è¾¨ç‡ {width}x{height}ï¼Œ{inference_steps} æ­¥æ¨ç†...')
                    sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
                    
                    if is_sdxl:
                        # SDXL æ¨ç†å‚æ•°
                        images = pipe(
                            prompt,
                            height=height,
                            width=width,
                            num_inference_steps=inference_steps,
                            guidance_scale=7.5,
                            num_images_per_prompt=batch_size,
                            # SDXL ç‰¹æœ‰å‚æ•°
                            original_size=(width, height),
                            target_size=(width, height)
                        ).images
                    else:
                        # æ ‡å‡† SD æ¨ç†å‚æ•°
                        images = pipe(
                            prompt,
                            height=height,
                            width=width,
                            num_inference_steps=inference_steps,
                            guidance_scale=7.5,
                            num_images_per_prompt=batch_size
                        ).images
                
                inference_end = time.time()
                inference_time = inference_end - inference_start
                
                print(f'âœ… æ¨ç†å®Œæˆï¼è€—æ—¶ {inference_time:.2f} ç§’')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
                print(f'Inference completed in {inference_time:.2f} seconds')
                print(f'Generated {len(images)} image(s)')
                print(f'Average time per image: {inference_time/len(images):.2f} seconds')
                
            except torch.cuda.OutOfMemoryError as e:
                print(f'âŒ CUDA Out of Memory Error detected!')
                print(f'Error details: {str(e)}')
                print(f'GPU Memory Status:')
                if torch.cuda.is_available():
                    gpu_memory_allocated = torch.cuda.memory_allocated() / (1024**3)
                    gpu_memory_cached = torch.cuda.memory_reserved() / (1024**3)
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    print(f'  Allocated: {gpu_memory_allocated:.2f}GB')
                    print(f'  Cached: {gpu_memory_cached:.2f}GB') 
                    print(f'  Total: {gpu_memory_total:.2f}GB')
                
                print(f'âš ï¸  Skipping test due to insufficient GPU memory')
                print(f'ğŸ’¡ Suggestions:')
                print(f'   - Use float16 instead of float32')
                print(f'   - Reduce batch_size to 1')
                print(f'   - Use a smaller resolution')
                print(f'   - Try a larger GPU instance type')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºï¼Œè®©è„šæœ¬èƒ½åŠæ—¶çœ‹åˆ°OOMä¿¡æ¯
                
                # åˆ›å»º OOM æµ‹è¯•ç»“æœè®°å½•
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_dir = f'/shared/stable-diffusion-outputs/{timestamp}_{test_name}_OOM'
                os.makedirs(output_dir, exist_ok=True)
                
                # ä¿å­˜ OOM æµ‹è¯•ç»“æœ
                oom_result = {
                    'test_info': {
                        'test_name': test_name,
                        'timestamp': timestamp,
                        'model': model_id,
                        'model_type': 'SDXL' if is_sdxl else 'Standard SD',
                        'instance_type': instance_type,
                        'batch_size': batch_size,
                        'inference_steps': inference_steps,
                        'resolution': f'{width}x{height}',
                        'prompt': prompt,
                        'precision': precision,
                        'torch_dtype': str(torch_dtype),
                        'status': 'OOM_FAILED'
                    },
                    'error_info': {
                        'error_type': 'CUDA_OUT_OF_MEMORY',
                        'error_message': str(e),
                        'model_load_time_seconds': round(model_load_time, 2) if 'model_load_time' in locals() else 'N/A'
                    },
                    'performance': {
                        'model_load_time_seconds': round(model_load_time, 2) if 'model_load_time' in locals() else 'N/A',
                        'inference_time_seconds': 'N/A',
                        'time_per_image_seconds': 'N/A',
                        'images_per_second': 'N/A',
                        'images_generated': 0,
                        'gpu_memory_allocated_gb': round(gpu_memory_allocated, 2) if torch.cuda.is_available() else 'N/A',
                        'gpu_memory_cached_gb': round(gpu_memory_cached, 2) if torch.cuda.is_available() else 'N/A',
                        'gpu_memory_total_gb': round(gpu_memory_total, 2) if torch.cuda.is_available() else 'N/A',
                        'gpu_memory_status': 'OOM'
                    },
                    'system_info': {
                        'python_version': platform.python_version(),
                        'torch_version': torch.__version__,
                        'cuda_available': torch.cuda.is_available(),
                        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',
                        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'
                    }
                }
                
                # ä¿å­˜ JSON ç»“æœ
                with open(f'{output_dir}/oom_test_result.json', 'w') as f:
                    json.dump(oom_result, f, indent=2)
                
                # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
                with open(f'{output_dir}/oom_test_report.txt', 'w') as f:
                    f.write(f'Stable Diffusion OOM Test Report\\n')
                    f.write(f'================================\\n\\n')
                    f.write(f'Test Configuration:\\n')
                    f.write(f'  Test Name: {test_name}\\n')
                    f.write(f'  Timestamp: {timestamp}\\n')
                    f.write(f'  Model: {model_id}\\n')
                    model_type_str = 'SDXL' if is_sdxl else 'Standard SD'
                    f.write(f'  Model Type: {model_type_str}\\n')
                    f.write(f'  Instance Type: {instance_type}\\n')
                    f.write(f'  Batch Size: {batch_size}\\n')
                    f.write(f'  Inference Steps: {inference_steps}\\n')
                    f.write(f'  Resolution: {width}x{height}\\n')
                    f.write(f'  Precision: {precision} ({torch_dtype})\\n')
                    f.write(f'  Prompt: {prompt}\\n\\n')
                    
                    f.write(f'Error Information:\\n')
                    f.write(f'  Error Type: CUDA Out of Memory\\n')
                    f.write(f'  Model Load Time: {round(model_load_time, 2) if \"model_load_time\" in locals() else \"N/A\"}s\\n')
                    f.write(f'  Error Message: {str(e)}\\n\\n')
                    
                    f.write(f'GPU Memory Status at OOM:\\n')
                    if torch.cuda.is_available():
                        f.write(f'  GPU Memory Allocated: {gpu_memory_allocated:.2f}GB\\n')
                        f.write(f'  GPU Memory Cached: {gpu_memory_cached:.2f}GB\\n')
                        f.write(f'  GPU Memory Total: {gpu_memory_total:.2f}GB\\n')
                        f.write(f'  GPU Memory Usage: {(gpu_memory_allocated/gpu_memory_total)*100:.1f}%\\n\\n')
                    
                    f.write(f'Recommendations:\\n')
                    f.write(f'  - Use float16 instead of float32 precision\\n')
                    f.write(f'  - Reduce batch_size to 1\\n')
                    f.write(f'  - Use a smaller resolution (e.g., 896x896 for SDXL)\\n')
                    f.write(f'  - Try a larger GPU instance type (e.g., g6e.xlarge)\\n')
                    f.write(f'  - Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\\n')
                
                print(f'OOM test result saved to: {output_dir}')
                
                # è¾“å‡º OOM æ ‡è®°åˆ°æ€§èƒ½æ‘˜è¦
                print(f'PERFORMANCE_SUMMARY: {test_name},{instance_type},{batch_size},{inference_steps},\"N/A\",\"N/A\",OOM')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
                
                # æ¸…ç† GPU å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ä¿æŒå®¹å™¨è¿è¡Œï¼Œè®©è„šæœ¬æœ‰æ—¶é—´æ£€æµ‹OOM
                print(f'Test {test_name} skipped due to OOM - keeping container alive for detection')
                print(f'ğŸ”„ å®¹å™¨å°†ä¿æŒè¿è¡Œ120ç§’ï¼Œä»¥ä¾¿å¤–éƒ¨è„šæœ¬æ£€æµ‹OOMçŠ¶æ€...')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºï¼Œè®©è„šæœ¬èƒ½åŠæ—¶çœ‹åˆ°OOMä¿æŒè¿è¡Œçš„ä¿¡æ¯
                
                # ä¿æŒå®¹å™¨è¿è¡Œ120ç§’ï¼Œè®©å¤–éƒ¨è„šæœ¬æœ‰è¶³å¤Ÿæ—¶é—´æ£€æµ‹OOM
                import time
                for i in range(120):
                    if i % 30 == 0:
                        print(f'â³ OOMå®¹å™¨ä¿æŒè¿è¡Œä¸­... å‰©ä½™ {120-i} ç§’')
                        sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºï¼Œè®©è„šæœ¬èƒ½å®æ—¶çœ‹åˆ°OOMå€’è®¡æ—¶
                    time.sleep(1)
                
                print(f'âœ… OOMæ£€æµ‹ç­‰å¾…å®Œæˆï¼Œå®¹å™¨å³å°†é€€å‡º')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
                exit(0)
                
            except Exception as e:
                print(f'âŒ Unexpected error during inference!')
                print(f'Error type: {type(e).__name__}')
                print(f'Error details: {str(e)}')
                
                # è¾“å‡ºé”™è¯¯æ ‡è®°åˆ°æ€§èƒ½æ‘˜è¦
                print(f'PERFORMANCE_SUMMARY: {test_name},{instance_type},{batch_size},{inference_steps},ERROR,ERROR,ERROR')
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
                
                # æ¸…ç† GPU å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                print(f'Test {test_name} failed - exiting with error')
                exit(1)
            print(f'Images per second: {len(images)/inference_time:.3f}')
            
            # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f'/shared/stable-diffusion-outputs/{timestamp}_{test_name}'
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜å›¾ç‰‡
            saved_files = []
            for i, image in enumerate(images):
                filename = f'{output_dir}/generated_image_{i}.png'
                image.save(filename)
                saved_files.append(filename)
                print(f'Saved image to {filename}')
            
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            gpu_memory_allocated = torch.cuda.memory_allocated()/1024**3 if torch.cuda.is_available() else 0
            gpu_memory_cached = torch.cuda.memory_reserved()/1024**3 if torch.cuda.is_available() else 0
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory/1024**3 if torch.cuda.is_available() else 0
            gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'
            
            # åˆ›å»ºæµ‹è¯•ç»“æœæŠ¥å‘Š
            test_results = {
                'test_info': {
                    'test_name': test_name,
                    'timestamp': timestamp,
                    'model': model_id,
                    'model_type': 'SDXL' if is_sdxl else 'Standard SD',
                    'instance_type': instance_type,
                    'batch_size': batch_size,
                    'inference_steps': inference_steps,
                    'resolution': f'{width}x{height}',
                    'prompt': prompt,
                    'precision': precision,
                    'torch_dtype': str(torch_dtype),
                    'scheduler': pipe.scheduler.__class__.__name__
                },
                'environment': {
                    'pytorch_version': torch.__version__,
                    'cuda_version': torch.version.cuda,
                    'gpu_name': gpu_name,
                    'gpu_memory_total_gb': round(gpu_memory_total, 2),
                    'hostname': os.environ.get('HOSTNAME', 'unknown')
                },
                'performance': {
                    'model_load_time_seconds': round(model_load_time, 2),
                    'inference_time_seconds': round(inference_time, 2),
                    'time_per_image_seconds': round(inference_time/len(images), 2),
                    'images_per_second': round(len(images)/inference_time, 3),
                    'images_generated': len(images),
                    'gpu_memory_allocated_gb': round(gpu_memory_allocated, 2),
                    'gpu_memory_cached_gb': round(gpu_memory_cached, 2),
                    'gpu_memory_utilization_percent': round((gpu_memory_allocated/gpu_memory_total)*100, 2) if gpu_memory_total > 0 else 0
                },
                'files': {
                    'output_directory': output_dir,
                    'saved_images': saved_files
                }
            }
            
            # ä¿å­˜JSONæ ¼å¼çš„æµ‹è¯•ç»“æœ
            results_file = f'{output_dir}/test_results.json'
            with open(results_file, 'w') as f:
                json.dump(test_results, f, indent=2)
            
            # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
            report_file = f'{output_dir}/test_report.txt'
            with open(report_file, 'w') as f:
                f.write(f'Universal Stable Diffusion Test Report\\n')
                f.write(f'=====================================\\n\\n')
                f.write(f'Test Configuration:\\n')
                f.write(f'  Test Name: {test_name}\\n')
                f.write(f'  Timestamp: {timestamp}\\n')
                f.write(f'  Model: {model_id}\\n')
                model_type_str = 'SDXL' if is_sdxl else 'Standard SD'
                f.write(f'  Model Type: {model_type_str}\\n')
                f.write(f'  Instance Type: {instance_type}\\n')
                f.write(f'  Batch Size: {batch_size}\\n')
                f.write(f'  Inference Steps: {inference_steps}\\n')
                f.write(f'  Resolution: {width}x{height}\\n')
                f.write(f'  Precision: {precision} ({torch_dtype})\\n')
                f.write(f'  Prompt: {prompt}\\n')
                f.write(f'  Scheduler: {pipe.scheduler.__class__.__name__}\\n\\n')
                
                f.write(f'Environment:\\n')
                f.write(f'  PyTorch Version: {torch.__version__}\\n')
                f.write(f'  CUDA Version: {torch.version.cuda}\\n')
                f.write(f'  GPU: {gpu_name}\\n')
                f.write(f'  GPU Memory: {gpu_memory_total:.1f}GB\\n')
                f.write(f'  Model Device: {pipe.device}\\n')
                f.write(f'  UNet Device: {pipe.unet.device}\\n\\n')
                
                f.write(f'Performance Results:\\n')
                f.write(f'  Model Load Time: {model_load_time:.2f}s\\n')
                f.write(f'  Inference Time: {inference_time:.2f}s\\n')
                f.write(f'  Time per Image: {inference_time/len(images):.2f}s\\n')
                f.write(f'  Images per Second: {len(images)/inference_time:.3f}\\n')
                f.write(f'  Images Generated: {len(images)}\\n')
                f.write(f'  GPU Memory Used: {gpu_memory_allocated:.2f}GB ({(gpu_memory_allocated/gpu_memory_total)*100:.1f}%)\\n')
                f.write(f'  GPU Memory Cached: {gpu_memory_cached:.2f}GB\\n\\n')
                
                f.write(f'Output Files:\\n')
                for file in saved_files:
                    f.write(f'  - {file}\\n')
            
            print(f'Test results saved to: {results_file}')
            print(f'Test report saved to: {report_file}')
            print(f'All outputs saved to: {output_dir}')
            print(f'Universal test {test_name} completed successfully!')
            
            # è¾“å‡ºå…³é”®æ€§èƒ½æŒ‡æ ‡åˆ°æ ‡å‡†è¾“å‡ºï¼ˆä¾¿äºè„šæœ¬è§£æï¼‰
            print(f'PERFORMANCE_SUMMARY: {test_name},{instance_type},{batch_size},{inference_steps},{inference_time:.2f},{inference_time/len(images):.2f},{gpu_memory_allocated:.2f}')
            sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒºï¼Œç¡®ä¿è„šæœ¬èƒ½åŠæ—¶æ£€æµ‹åˆ°
            
            # ä¿æŒå®¹å™¨è¿è¡Œï¼Œè®©è„šæœ¬æœ‰æ—¶é—´æŠ“å–æ—¥å¿—å’Œæ•°æ®
            print(f'âœ… æµ‹è¯•å®Œæˆï¼Œå®¹å™¨å°†ä¿æŒè¿è¡Œ180ç§’ä»¥ä¾¿è„šæœ¬æŠ“å–æ•°æ®...')
            print(f'ğŸ”„ è„šæœ¬å¯ä»¥åœ¨æ­¤æœŸé—´æŠ“å–æ—¥å¿—ã€æ€§èƒ½æ•°æ®å¹¶ä¸»åŠ¨å–æ¶ˆä»»åŠ¡')
            sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºï¼Œè®©è„šæœ¬èƒ½çœ‹åˆ°è¿™äº›æç¤ºä¿¡æ¯
            
            # ä¿æŒå®¹å™¨è¿è¡Œ180ç§’ï¼Œè®©å¤–éƒ¨è„šæœ¬æœ‰è¶³å¤Ÿæ—¶é—´å¤„ç†
            import time
            for i in range(180):
                if i % 60 == 0:
                    print(f'â³ ä»»åŠ¡å®Œæˆï¼Œç­‰å¾…è„šæœ¬å¤„ç†... å‰©ä½™ {180-i} ç§’')
                    sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡ºï¼Œè®©è„šæœ¬èƒ½å®æ—¶çœ‹åˆ°å€’è®¡æ—¶
                time.sleep(1)
            
            print(f'âœ… ç­‰å¾…æ—¶é—´ç»“æŸï¼Œå®¹å™¨å³å°†é€€å‡º')
            sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
            "
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "${MEMORY_LIMIT}"
            cpu: "4"
          requests:
            nvidia.com/gpu: 1
            memory: "${MEMORY_REQUEST}"
            cpu: "2"
        env:
        - name: TEST_NAME
          value: "${TEST_NAME}"
        - name: MODEL_ID
          value: "${MODEL_ID}"
        - name: INSTANCE_TYPE
          value: "${INSTANCE_TYPE}"
        - name: BATCH_SIZE
          value: "${BATCH_SIZE}"
        - name: INFERENCE_STEPS
          value: "${INFERENCE_STEPS}"
        - name: PROMPT
          value: "${PROMPT}"
        - name: IMAGE_WIDTH
          value: "${IMAGE_WIDTH}"
        - name: IMAGE_HEIGHT
          value: "${IMAGE_HEIGHT}"
        - name: PRECISION
          value: "${PRECISION}"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:128"
        volumeMounts:
        - name: efs-storage
          mountPath: /shared
        - name: tmp-storage
          mountPath: /tmp
      volumes:
      - name: efs-storage
        persistentVolumeClaim:
          claimName: efs-sc-pvc
      - name: tmp-storage
        emptyDir:
          sizeLimit: 8Gi
